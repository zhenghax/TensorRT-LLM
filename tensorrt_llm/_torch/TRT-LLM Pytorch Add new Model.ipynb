{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00020ae7-fe6c-4c31-bc97-0f01e6544d76",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  python3-distro-info\n",
      "Use 'sudo apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in ./.local/lib/python3.10/site-packages (25.0.1)\n",
      "Requirement already satisfied: setuptools in ./.local/lib/python3.10/site-packages (78.0.2)\n",
      "\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Requirement already satisfied: tensorrt_llm in ./.local/lib/python3.10/site-packages (0.17.0.post1)\n",
      "Collecting flashinfer@ git+https://github.com/flashinfer-ai/flashinfer.git@06309c4e (from tensorrt_llm)\n",
      "  Cloning https://github.com/flashinfer-ai/flashinfer.git (to revision 06309c4e) to /tmp/pip-install-lkoylxxz/flashinfer_ca2212c3812c48289af970cd2c223eb0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/flashinfer-ai/flashinfer.git /tmp/pip-install-lkoylxxz/flashinfer_ca2212c3812c48289af970cd2c223eb0\n",
      "\u001b[33m  WARNING: Did not find branch or tag '06309c4e', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running command git checkout -q 06309c4e\n",
      "  Resolved https://github.com/flashinfer-ai/flashinfer.git to commit 06309c4e\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting s2wrapper@ git+https://github.com/bfshi/scaling_on_scales.git@60da2afe (from tensorrt_llm)\n",
      "  Cloning https://github.com/bfshi/scaling_on_scales.git (to revision 60da2afe) to /tmp/pip-install-lkoylxxz/s2wrapper_f8179d1ffda64b068473e895430f445a\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/bfshi/scaling_on_scales.git /tmp/pip-install-lkoylxxz/s2wrapper_f8179d1ffda64b068473e895430f445a\n",
      "\u001b[33m  WARNING: Did not find branch or tag '60da2afe', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running command git checkout -q 60da2afe\n",
      "  Resolved https://github.com/bfshi/scaling_on_scales.git to commit 60da2afe\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.25.0 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (1.5.2)\n",
      "Requirement already satisfied: build in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (1.2.2.post1)\n",
      "Requirement already satisfied: colored in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (2.3.0)\n",
      "Requirement already satisfied: cuda-python in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (12.8.0)\n",
      "Requirement already satisfied: diffusers>=0.27.0 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (0.32.2)\n",
      "Requirement already satisfied: lark in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (1.2.2)\n",
      "Requirement already satisfied: mpi4py in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (4.0.3)\n",
      "Requirement already satisfied: numpy<2 in /usr/lib/python3/dist-packages (from tensorrt_llm) (1.21.5)\n",
      "Requirement already satisfied: onnx>=1.12.0 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (1.17.0)\n",
      "Requirement already satisfied: onnx-graphsurgeon>=0.5.2 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (0.5.6)\n",
      "Requirement already satisfied: openai in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (1.68.2)\n",
      "Requirement already satisfied: polygraphy in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (0.49.20)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from tensorrt_llm) (5.9.0)\n",
      "Requirement already satisfied: pynvml>=11.5.0 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (12.0.0)\n",
      "Requirement already satisfied: pulp in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (3.1.1)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from tensorrt_llm) (1.3.5)\n",
      "Requirement already satisfied: h5py==3.12.1 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (3.12.1)\n",
      "Requirement already satisfied: StrEnum in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (0.4.15)\n",
      "Requirement already satisfied: sentencepiece>=0.1.99 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (0.2.0)\n",
      "Requirement already satisfied: tensorrt~=10.8.0 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (10.8.0.43)\n",
      "Requirement already satisfied: torch<=2.6.0a0,>=2.5.1 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (2.5.1)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (0.20.1)\n",
      "Requirement already satisfied: nvidia-modelopt~=0.23.0 in ./.local/lib/python3.10/site-packages (from nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (0.23.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (12.4.127)\n",
      "Requirement already satisfied: transformers<4.48.0,>=4.47.0 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (4.47.1)\n",
      "Requirement already satisfied: pydantic>=2.9.1 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (2.10.6)\n",
      "Requirement already satisfied: pillow==10.3.0 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (10.3.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from tensorrt_llm) (0.37.1)\n",
      "Requirement already satisfied: optimum in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (1.24.0)\n",
      "Requirement already satisfied: evaluate in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (0.4.3)\n",
      "Requirement already satisfied: mpmath>=1.3.0 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (1.3.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from tensorrt_llm) (8.0.3)\n",
      "Requirement already satisfied: click-option-group in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (0.5.7)\n",
      "Requirement already satisfied: aenum in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (3.1.15)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm) (26.3.0)\n",
      "Requirement already satisfied: fastapi==0.115.4 in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (0.115.4)\n",
      "Requirement already satisfied: uvicorn in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (0.34.0)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm) (0.28.1)\n",
      "Requirement already satisfied: setuptools in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (78.0.2)\n",
      "Requirement already satisfied: ordered-set in ./.local/lib/python3.10/site-packages (from tensorrt_llm) (4.1.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in ./.local/lib/python3.10/site-packages (from fastapi==0.115.4->tensorrt_llm) (0.41.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.10/site-packages (from fastapi==0.115.4->tensorrt_llm) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->tensorrt_llm) (24.2)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.25.0->tensorrt_llm) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.local/lib/python3.10/site-packages (from accelerate>=0.25.0->tensorrt_llm) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from accelerate>=0.25.0->tensorrt_llm) (0.5.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers>=0.27.0->tensorrt_llm) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from diffusers>=0.27.0->tensorrt_llm) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from diffusers>=0.27.0->tensorrt_llm) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers>=0.27.0->tensorrt_llm) (2.32.3)\n",
      "Requirement already satisfied: nvidia-modelopt-core==0.23.2 in ./.local/lib/python3.10/site-packages (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (0.23.2)\n",
      "Requirement already satisfied: cloudpickle>=1.6.0 in ./.local/lib/python3.10/site-packages (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (3.1.1)\n",
      "Requirement already satisfied: ninja in ./.local/lib/python3.10/site-packages (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (1.11.1.4)\n",
      "Requirement already satisfied: rich in /usr/lib/python3/dist-packages (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (11.2.0)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (1.8.0)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (4.67.1)\n",
      "Requirement already satisfied: torchprofile>=0.0.4 in ./.local/lib/python3.10/site-packages (from nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (0.0.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/lib/python3/dist-packages (from onnx>=1.12.0->tensorrt_llm) (4.21.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.local/lib/python3.10/site-packages (from pydantic>=2.9.1->tensorrt_llm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.local/lib/python3.10/site-packages (from pydantic>=2.9.1->tensorrt_llm) (2.27.2)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/lib/python3/dist-packages (from pynvml>=11.5.0->tensorrt_llm) (12.555.43)\n",
      "Requirement already satisfied: tensorrt_cu12==10.8.0.43 in ./.local/lib/python3.10/site-packages (from tensorrt~=10.8.0->tensorrt_llm) (10.8.0.43)\n",
      "Requirement already satisfied: tensorrt_cu12_libs==10.8.0.43 in ./.local/lib/python3.10/site-packages (from tensorrt_cu12==10.8.0.43->tensorrt~=10.8.0->tensorrt_llm) (10.8.0.43)\n",
      "Requirement already satisfied: tensorrt_cu12_bindings==10.8.0.43 in ./.local/lib/python3.10/site-packages (from tensorrt_cu12==10.8.0.43->tensorrt~=10.8.0->tensorrt_llm) (10.8.0.43)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12 in ./.local/lib/python3.10/site-packages (from tensorrt_cu12_libs==10.8.0.43->tensorrt_cu12==10.8.0.43->tensorrt~=10.8.0->tensorrt_llm) (12.4.127)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (2.4)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /usr/lib/python3/dist-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.10/site-packages (from torch<=2.6.0a0,>=2.5.1->tensorrt_llm) (1.13.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.10/site-packages (from transformers<4.48.0,>=4.47.0->tensorrt_llm) (0.21.1)\n",
      "Requirement already satisfied: pyproject_hooks in ./.local/lib/python3.10/site-packages (from build->tensorrt_llm) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->tensorrt_llm) (2.2.1)\n",
      "Requirement already satisfied: cuda-bindings~=12.8.0 in ./.local/lib/python3.10/site-packages (from cuda-python->tensorrt_llm) (12.8.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.local/lib/python3.10/site-packages (from evaluate->tensorrt_llm) (3.4.1)\n",
      "Requirement already satisfied: dill in ./.local/lib/python3.10/site-packages (from evaluate->tensorrt_llm) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from evaluate->tensorrt_llm) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.10/site-packages (from evaluate->tensorrt_llm) (0.70.16)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->tensorrt_llm) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx->tensorrt_llm) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->tensorrt_llm) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx->tensorrt_llm) (3.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->tensorrt_llm) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->tensorrt_llm) (1.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.local/lib/python3.10/site-packages (from openai->tensorrt_llm) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->tensorrt_llm) (1.3.1)\n",
      "Requirement already satisfied: einops in ./.local/lib/python3.10/site-packages (from s2wrapper@ git+https://github.com/bfshi/scaling_on_scales.git@60da2afe->tensorrt_llm) (0.8.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->tensorrt_llm) (1.2.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->tensorrt_llm) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->tensorrt_llm) (3.11.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers>=0.27.0->tensorrt_llm) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->diffusers>=0.27.0->tensorrt_llm) (1.26.5)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/lib/python3/dist-packages (from rich->nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (0.4.4)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/lib/python3/dist-packages (from rich->nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/lib/python3/dist-packages (from rich->nvidia-modelopt~=0.23.0->nvidia-modelopt[torch]~=0.23.0->tensorrt_llm) (2.11.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (1.18.3)\n",
      "\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!sudo apt-get -y install libopenmpi-dev && pip3 install --upgrade pip setuptools && pip3 install tensorrt_llm --extra-index-url https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8430397-d674-41c0-91fe-8562cf9efdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'TensorRT-LLM' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/TensorRT-LLM.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d52a8d92-5a3a-445f-b6c9-1730f16892a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/TensorRT-LLM/tensorrt_llm/_torch/models\n"
     ]
    }
   ],
   "source": [
    "cd TensorRT-LLM/tensorrt_llm/_torch/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7363ece-5901-43a9-a484-7134eacdbefc",
   "metadata": {},
   "source": [
    "tensorrt_llm/_torch/models/modeling_llama.py: ~200 LoC\n",
    "\n",
    "LlamaAttention: ~30 LoC\n",
    "\n",
    "LlamaDecoderLayer: ~60 LoC\n",
    "\n",
    "LlamaModel: ~50 LoC\n",
    "\n",
    "LlamaForCausalLM: ~15 LoC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3867a05-5f9f-4356-8b97-f75f2bc3a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-25 22:09:21.299926: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 22:09:21.487555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742940561.567752   93479 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742940561.591223   93479 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-25 22:09:21.761736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.17.0.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import LlamaConfig\n",
    "from tensorrt_llm.functional import PositionEmbeddingType\n",
    "\n",
    "from tensorrt_llm._torch.attention_backend import AttentionMetadata\n",
    "from tensorrt_llm._torch.attention_backend.interface import PositionalEmbeddingParams, RopeParams\n",
    "from tensorrt_llm._torch.distributed import ParallelConfig, TensorParallelMode\n",
    "from tensorrt_llm._torch.model_config import ModelConfig\n",
    "from tensorrt_llm._torch.modules.attention import Attention\n",
    "from tensorrt_llm._torch.modules.decoder_layer import DecoderLayer\n",
    "from tensorrt_llm._torch.modules.embedding import Embedding\n",
    "from tensorrt_llm._torch.modules.gated_mlp import GatedMLP\n",
    "from tensorrt_llm._torch.modules.rms_norm import RMSNorm\n",
    "from tensorrt_llm._torch.modules.rotary_embedding import RotaryEmbedding\n",
    "from tensorrt_llm._torch.models.modeling_utils import (DecoderModel, DecoderModelForCausalLM,\n",
    "                             register_auto_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d48cb0-094e-4b80-a7b7-f56c3b75d095",
   "metadata": {},
   "source": [
    "Note that MyAttention inherits from our Attention module (in tensorrt_llm/_torch/modules/attention.py), so that the attention computation is compatible with our PyTorch runtime. Related to this, module inputs should also be adapted:\n",
    "\n",
    "The attn_metadata stores the metadata from the batched input and KV cache for the attention backend. It is created by and passed from the runtime, and model developers need to ensure that attn_metadata is correctly passed to the attention module.\n",
    "\n",
    "The input tensors (i.e., input_ids, position_ids, hidden_states) are in the packed mode. The first dimension corresponds to the number of tokens in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395a0178-3a6c-4b9c-a109-4fe7d98c73a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "Name: tensorrt-llm\n",
      "Version: 0.17.0.post1\n",
      "Summary: TensorRT-LLM: A TensorRT Toolbox for Large Language Models\n",
      "Home-page: https://github.com/NVIDIA/TensorRT-LLM\n",
      "Author: NVIDIA Corporation\n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: /home/ubuntu/.local/lib/python3.10/site-packages\n",
      "Requires: accelerate, aenum, build, click, click-option-group, colored, cuda-python, diffusers, evaluate, fastapi, flashinfer, h5py, httpx, lark, mpi4py, mpmath, numpy, nvidia-cuda-nvrtc-cu12, nvidia-modelopt, nvidia-nccl-cu12, onnx, onnx-graphsurgeon, openai, optimum, ordered-set, pandas, pillow, polygraphy, psutil, pulp, pydantic, pynvml, pyzmq, s2wrapper, sentencepiece, setuptools, StrEnum, tensorrt, torch, torchvision, transformers, uvicorn, wheel\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!pip show tensorrt-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68745c27-fe62-4c3b-88e5-645abd600a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttention(Attention):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: ModelConfig[LlamaConfig],\n",
    "        layer_idx: Optional[int] = None,\n",
    "    ):\n",
    "        config = model_config.pretrained_config\n",
    "        if model_config.fuse_pos_embd:\n",
    "            pos_embd_params = PositionalEmbeddingParams(\n",
    "                type=PositionEmbeddingType.rope_gpt_neox,\n",
    "                rope=RopeParams.from_config(config),\n",
    "            )\n",
    "        else:\n",
    "            pos_embd_params = None\n",
    "        super().__init__(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            num_key_value_heads=config.num_key_value_heads,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            bias=config.attention_bias,\n",
    "            rotary_emb=LlamaRotaryEmbedding(config),\n",
    "            pos_embd_params=pos_embd_params,\n",
    "            layer_idx=layer_idx,\n",
    "            dtype=config.torch_dtype,\n",
    "            config=model_config,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279648c-4303-41ab-adee-b2217633a7df",
   "metadata": {},
   "source": [
    "Additionally, MyDecoderLayer, MyModel, and MyModelForCausalLM are subclasses of DecoderLayer, DecoderModel, and DecoderModelForCausalLM respectively. The base classes define interfaces and provide a generic scaffolding to define model layers, load weights, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13621983-2f68-4494-a2e2-1fc1019c8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(DecoderLayer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: ModelConfig[LlamaConfig],\n",
    "        layer_idx: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        super().__init__()\n",
    "        config = model_config.pretrained_config\n",
    "        self.self_attn = LlamaAttention(\n",
    "            model_config,\n",
    "            layer_idx=layer_idx,\n",
    "        )\n",
    "\n",
    "        self.mlp = GatedMLP(\n",
    "            hidden_size=config.hidden_size,\n",
    "            intermediate_size=config.intermediate_size,\n",
    "            bias=config.mlp_bias,\n",
    "            dtype=config.torch_dtype,\n",
    "            config=model_config,\n",
    "        )\n",
    "        self.input_layernorm = RMSNorm(hidden_size=config.hidden_size,\n",
    "                                       eps=config.rms_norm_eps,\n",
    "                                       dtype=config.torch_dtype)\n",
    "        self.post_attention_layernorm = RMSNorm(hidden_size=config.hidden_size,\n",
    "                                                eps=config.rms_norm_eps,\n",
    "                                                dtype=config.torch_dtype)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        position_ids: torch.LongTensor,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attn_metadata: AttentionMetadata,\n",
    "        residual: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        if residual is None:\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.input_layernorm(hidden_states)\n",
    "        else:\n",
    "            hidden_states, residual = self.input_layernorm(\n",
    "                hidden_states, residual)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states = self.self_attn(\n",
    "            position_ids=position_ids,\n",
    "            hidden_states=hidden_states,\n",
    "            attn_metadata=attn_metadata,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Fully Connected\n",
    "        hidden_states, residual = self.post_attention_layernorm(\n",
    "            hidden_states, residual)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        return hidden_states, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd839ce-044a-4dec-b1f3-640a50de38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(DecoderModel):\n",
    "\n",
    "    def __init__(self, model_config: ModelConfig[LlamaConfig]):\n",
    "        super().__init__(model_config)\n",
    "        config = self.model_config.pretrained_config\n",
    "        self.padding_idx = config.pad_token_id\n",
    "\n",
    "        self.embed_tokens = Embedding(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            dtype=config.torch_dtype,\n",
    "            parallel_config=ParallelConfig(\n",
    "                tensor_parallel_rank=model_config.mapping.tp_rank,\n",
    "                tensor_parallel_size=model_config.mapping.tp_size,\n",
    "                tensor_parallel_mode=TensorParallelMode.COLUMN,\n",
    "                gather_output=True,\n",
    "                gpus_per_node=model_config.mapping.gpus_per_node,\n",
    "            ),\n",
    "        )\n",
    "        self.layers = nn.ModuleList([\n",
    "            LlamaDecoderLayer(\n",
    "                model_config,\n",
    "                layer_idx,\n",
    "            ) for layer_idx in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.norm = RMSNorm(hidden_size=config.hidden_size,\n",
    "                            eps=config.rms_norm_eps,\n",
    "                            dtype=config.torch_dtype)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        attn_metadata: AttentionMetadata,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        residual = None\n",
    "        for decoder_layer in self.layers:\n",
    "            hidden_states, residual = decoder_layer(position_ids=position_ids,\n",
    "                                                    hidden_states=hidden_states,\n",
    "                                                    attn_metadata=attn_metadata,\n",
    "                                                    residual=residual)\n",
    "\n",
    "        hidden_states, _ = self.norm(hidden_states, residual)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa6835-674c-41f0-b513-8c0a438047f0",
   "metadata": {},
   "source": [
    "Optionally, you may replace the native PyTorch modules with our implementations to enable features or achieve higher performance:\n",
    "\n",
    "Linear (in tensorrt_llm/_torch/modules/linear.py): Enables tensor parallelism and quantization.\n",
    "\n",
    "Embedding (in tensorrt_llm/_torch/modules/embedding.py): Enables tensor parallelism for embedding.\n",
    "\n",
    "RotaryEmbedding (in tensorrt_llm/_torch/modules/rotary_embedding.py): Enables performant rotary embedding.\n",
    "\n",
    "RMSNorm (in tensorrt_llm/_torch/modules/rms_norm.py): Enables performant RMS norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28fb90f2-1db9-4efe-b809-6aa717915d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRotaryEmbedding(RotaryEmbedding):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: LlamaConfig,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            config,\n",
    "            head_dim=config.hidden_size // config.num_attention_heads,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            device=device,\n",
    "            rope_type=\"default\" if config.rope_scaling is None else \"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa56be8-515b-48b3-80ba-a8cded315c59",
   "metadata": {},
   "source": [
    "Model Registration\n",
    "\n",
    "The new model needs to be registered so that it can be recognized by the PyTorch runtime. The registration can be done simply by adding the register_auto_model decorator for MyModelForCausalLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f2105e1-a555-40f3-8625-61d7d1faab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_auto_model(\"LlamaForCausalLM\")\n",
    "class LlamaForCausalLM(DecoderModelForCausalLM[LlamaModel, LlamaConfig]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: ModelConfig[LlamaConfig],\n",
    "    ):\n",
    "        super().__init__(LlamaModel(model_config),\n",
    "                         config=model_config,\n",
    "                         hidden_size=model_config.pretrained_config.hidden_size,\n",
    "                         vocab_size=model_config.pretrained_config.vocab_size)\n",
    "\n",
    "\n",
    "@register_auto_model(\"MistralForCausalLM\")\n",
    "class MistralForCausalLM(DecoderModelForCausalLM[LlamaModel, LlamaConfig]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: ModelConfig[LlamaConfig],\n",
    "    ):\n",
    "        super().__init__(LlamaModel(model_config),\n",
    "                         config=model_config,\n",
    "                         hidden_size=model_config.pretrained_config.hidden_size,\n",
    "                         vocab_size=model_config.pretrained_config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4c46b-f1ac-441d-b02e-01246432e0ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
